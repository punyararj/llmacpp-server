{
  "host": "0.0.0.0",
  "port": 8080,
  "models": [
    {
      "model": "/app/models/llama-2-7b-chat.Q4_K_M.gguf",
      "model_alias": "Llama2",
      "chat_format": "chatml",
      "n_gpu_layers": 20,
      "offload_kqv": true,
      "n_threads": 12,
      "n_batch": 512,
      "n_ctx": 4096
    }
  ]
}
